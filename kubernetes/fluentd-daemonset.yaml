apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd-daemonset
  namespace: kube-system
  labels:
    k8s-app: fluentd-daemonset
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
spec:
  selector:
    matchLabels:
      k8s-app: fluentd-daemonset
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        kubernetes.io/cluster-service: "true"
        k8s-app: fluentd-daemonset
      # This annotation ensures that fluentd does not get evicted if the node
      # supports critical pod annotation based priority scheme.
      # Note that this does not guarantee admission on the nodes (#40573).
      annotations:
        scheduler.alpha.kubernetes.io/critical-pod: ''
    spec:
      priorityClassName: system-node-critical
      serviceAccountName: default
      dnsPolicy: Default
      hostNetwork: true
      restartPolicy: Always
      terminationGracePeriodSeconds: 30
      containers:
      - name: fluentd
        # https://console.cloud.google.com/gcr/images/stackdriver-agents/GLOBAL/stackdriver-logging-agent?gcrImageListsize=30
        image: sandbox/fluentd
        securityContext:
          runAsUser: 0
        command: ["fluentd", "-c", "/fluentd/etc/fluent.conf"]
        volumeMounts:
        - name: var-log
          mountPath: /var/log
          readOnly: true
        - name: var-lib-fluentd
          mountPath: /var/lib/fluentd
          readOnly: false
        - name: var-lib-docker-containers
          mountPath: /var/lib/docker/containers
          readOnly: true
        - name: fluentd-config-volume
          mountPath: /fluentd/etc
        - name: fluentd-config-directory-volume
          mountPath: /fluentd/etc/config.d
        - name: google-application-credentials
          mountPath: /etc/google/auth
        env:
        - name: GOOGLE_APPLICATION_CREDENTIALS
          value: "/etc/google/auth/application_default_credentials.json"
        - name: K8S_NODE_CONSTANTS
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: spec.nodeName
        # Liveness probe is aimed to help in situarions where fluentd
        # silently hangs for no apparent reasons until manual restart.
        # The idea of this probe is that if fluentd is not queueing or
        # flushing chunks for 5 minutes, something is not right. If
        # you want to change the fluentd configuration, reducing amount of
        # logs fluentd collects, consider changing the threshold or turning
        # liveness probe off completely.
        # livenessProbe:
        #   initialDelaySeconds: 600
        #   periodSeconds: 60
        #   exec:
        #     command:
        #     - '/bin/sh'
        #     - '-c'
        #     - >
        #       LIVENESS_THRESHOLD_SECONDS=${LIVENESS_THRESHOLD_SECONDS:-300};
        #       STUCK_THRESHOLD_SECONDS=${LIVENESS_THRESHOLD_SECONDS:-900};
        #       if [ ! -e /var/log/fluentd-buffers ];
        #       then
        #         exit 1;
        #       fi;
        #       touch -d "${STUCK_THRESHOLD_SECONDS} seconds ago" /tmp/marker-stuck;
        #       if [[ -z "$(find /var/log/fluentd-buffers -type f -newer /tmp/marker-stuck -print -quit)" ]];
        #       then
        #         rm -rf /var/log/fluentd-buffers;
        #         exit 1;
        #       fi;
        #       touch -d "${LIVENESS_THRESHOLD_SECONDS} seconds ago" /tmp/marker-liveness;
        #       if [[ -z "$(find /var/log/fluentd-buffers -type f -newer /tmp/marker-liveness -print -quit)" ]];
        #       then
        #         exit 1;
        #       fi;
      # - name: prometheus-to-sd-exporter
      #   image: k8s.gcr.io/prometheus-to-sd:v0.3.1
      #   command:
      #     - /monitor
      #     - --stackdriver-prefix=prometheus/addons
      #     - --api-override={{ prometheus_to_sd_endpoint }}
      #     - --source=fluentd:http://localhost:24231?whitelisted=stackdriver_successful_requests_count,stackdriver_failed_requests_count,stackdriver_ingested_entries_count,stackdriver_dropped_entries_count
      #     - --pod-id=$(POD_NAME)
      #     - --namespace-id=$(POD_NAMESPACE)
      #   env:
      #     - name: POD_NAME
      #       valueFrom:
      #         fieldRef:
      #           fieldPath: metadata.name
      #     - name: POD_NAMESPACE
      #       valueFrom:
      #         fieldRef:
      #           fieldPath: metadata.namespace
      # terminationGracePeriodSeconds: 60
      # tolerations:
      # - operator: "Exists"
      #   effect: "NoExecute"
      # - operator: "Exists"
      #   effect: "NoSchedule"
      volumes:
      - name: var-log
        hostPath:
          path: /var/log
      - name: var-lib-fluentd
        hostPath:
          path: /var/lib/fluentd
      - name: var-lib-docker-containers
        hostPath:
          path: /var/lib/docker/containers
      - name: fluentd-config-volume
        configMap:
          name: fluentd-config
      - name: fluentd-config-directory-volume
        configMap:
          name: fluentd-config-directory
      - name: google-application-credentials
        secret:
          secretName: google-application-credentials
---
kind: ConfigMap
apiVersion: v1
data:
  fluent.conf: |-
    @include config.d/*.conf

    # # Configure all sources to output to Google Cloud Logging
    # <match **>
    #   @type google_cloud
    #   # Set the chunk limit conservatively to avoid exceeding the recommended
    #   # chunk size of 5MB per write request.
    #   buffer_chunk_limit 1M
    #   # Flush logs every 5 seconds, even if the buffer is not full.
    #   flush_interval 5s
    #   # Enforce some limit on the number of retries.
    #   disable_retry_limit false
    #   # After 3 retries, a given chunk will be discarded.
    #   retry_limit 3
    #   # Wait 10 seconds before the first retry. The wait interval will be doubled on
    #   # each following retry (20s, 40s...) until it hits the retry limit.
    #   retry_wait 10
    #   # Never wait longer than 5 minutes between retries. If the wait interval
    #   # reaches this limit, the exponentiation stops.
    #   # Given the default config, this limit should never be reached, but if
    #   # retry_limit and retry_wait are customized, this limit might take effect.
    #   max_retry_wait 300
    #   # Use multiple threads for processing.
    #   num_threads 8
    #   detect_json true
    #   # Enable metadata agent lookups.
    #   enable_metadata_agent true
    #   metadata_agent_url "http://local-metadata-agent.stackdriver.com:8000"
    #   # Use the gRPC transport.
    #   use_grpc true
    #   # If a request is a mix of valid log entries and invalid ones, ingest the
    #   # valid ones and drop the invalid ones instead of dropping everything.
    #   partial_success true
    #   # Enable monitoring via Prometheus integration.
    #   enable_monitoring true
    #   monitoring_type prometheus
    # </match>
metadata:
  name: fluentd-config
  namespace: kube-system
---
kind: ConfigMap
apiVersion: v1
data:
  containers.input.conf: |-
    # The original tag is derived from the log file's location.
    # For example a Docker container's logs might be in the directory:
    #  /var/lib/docker/containers/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b
    # and in the file:
    #  997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b-json.log
    # where 997599971ee6... is the Docker ID of the running container.
    # The Kubernetes kubelet makes a symbolic link to this file on the host
    # machine in the /var/log/containers directory which includes the pod name,
    # the namespace name and the Kubernetes container name:
    #    synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log
    #    ->
    #    /var/lib/docker/containers/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b-json.log
    # The /var/log directory on the host is mapped to the /var/log directory in the container
    # running this instance of Fluentd and we end up collecting the file:
    #   /var/log/containers/synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log
    # This results in the tag:
    #  var.log.containers.synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log
    # where 'synthetic-logger-0.25lps-pod' is the pod name, 'default' is the
    # namespace name, 'synth-lgr' is the container name and '997599971ee6..' is
    # the container ID.
    <source>
      @type tail
      path /var/log/containers/*.log
      pos_file /var/lib/fluentd/containers.log.pos
      time_format %Y-%m-%dT%H:%M:%S.%NZ
      tag kubernetes.*
      format json
      read_from_head true
    </source>

    <match fluent.**>
      @type null
    </match>

    <filter kubernetes.**>
      @type kubernetes_metadata
    </filter>

    <match kubernetes.var.log.containers.**fluentd**.log>
      @type null
    </match>

    <match kubernetes.var.log.containers.**kube-system**.log>
      @type null
    </match>

    <filter kubernetes.var.log.containers.**>
      @type record_transformer
      enable_ruby true
      renew_record false
      <record>
        log ${record['log'].strip}
        tag ${tag}
      </record>
    </filter>

    <filter kubernetes.var.log.containers.**logs-generator**>
      @type parser
      key_name log
      reserve_time true
      reserve_data true
      <parse>
        @type tsv
        keys time,sequence,method,url,req_secs
        delimiter " "
      </parse>
    </filter>

    <match kubernetes.var.log.containers.**>
      @type record_transformer
      enable_ruby true
      <record>
        logging.googleapis.com/local_resource_id ${"k8s_container.${kubernetes['namespace_name']}.${kubernetes['pod_name']}.${kubernetes['container_name']}"}
        message ${record['log']}
        severity ${record['severity'] || if record['stream'] == 'stderr' then 'ERROR' else 'INFO' end}
      </record>
      tag ${if record['stream'] == 'stderr' then 'raw.stderr' else 'raw.stdout' end}
      remove_keys kubernetes,labels,docker,stream,tag,log
    </match>

    # Detect exceptions in the log output and forward them as one log entry.
    <match {raw.stderr,raw.stdout}>
      @type detect_exceptions

      remove_tag_prefix raw
      message message
      stream "logging.googleapis.com/local_resource_id"
      multiline_flush_interval 5
      max_bytes 500000
      max_lines 1000
    </match>
  system.input.conf: |-
    <source>
      @type tail
      format /^time="(?<time>[^)]*)" level=(?<severity>[^ ]*) msg="(?<message>[^"]*)"( err="(?<error>[^"]*)")?( statusCode=($<status_code>\d+))?/
      path /var/log/docker.log
      pos_file /var/lib/fluentd-docker.log.pos
      tag docker
    </source>

  monitoring.conf: |-
    # This source is used to acquire approximate process start timestamp,
    # which purpose is explained before the corresponding output plugin.
    <source>
      @type exec
      command /bin/sh -c 'date +%s'
      tag process_start
      time_format %Y-%m-%d %H:%M:%S
      keys process_start_timestamp
    </source>

    # This filter is used to convert process start timestamp to integer
    # value for correct ingestion in the prometheus output plugin.
    <filter process_start>
      @type record_transformer
      enable_ruby true
      auto_typecast true
      <record>
        process_start_timestamp ${record["process_start_timestamp"].to_i}
      </record>
    </filter>
  output.conf: |-
    <match process_start>
      @type prometheus
      <metric>
        type gauge
        name process_start_time_seconds
        desc Timestamp of the process start in seconds
        key process_start_timestamp
      </metric>
    </match>

    # This filter allows to count the number of log entries read by fluentd
    # before they are processed by the output plugin. This in turn allows to
    # monitor the number of log entries that were read but never sent, e.g.
    # because of liveness probe removing buffer.
    <filter **>
      @type prometheus
      <metric>
        type counter
        name logging_entry_count
        desc Total number of log entries generated by either application containers or system components
      </metric>
    </filter>

    # This section is exclusive for k8s_container logs. Those come with
    # 'stderr'/'stdout' tags.
    # TODO(instrumentation): Reconsider this workaround later.
    # Trim the entries which exceed slightly less than 100KB, to avoid
    # dropping them. It is a necessity, because Stackdriver only supports
    # entries that are up to 100KB in size.
    <filter {stderr,stdout}>
      @type record_transformer
      enable_ruby true
      <record>
        message ${record['message'].length > 10000 ? "[Trimmed]#{record['message'][0..10000]}..." : record['message']}
      </record>
    </filter>

    <filter **>
      @type add_insert_ids
    </filter>

    <match {stderr,stdout}>
      @type google_cloud

      detect_json true
      enable_monitoring true
      monitoring_type prometheus
      split_logs_by_tag false
      # Set the buffer type to file to improve the reliability and reduce the memory consumption
      buffer_type file
      buffer_path /var/lib/fluentd/buffer/google-cloud-std.buf
      # Set queue_full action to block because we want to pause gracefully
      # in case of the off-the-limits load instead of throwing an exception
      buffer_queue_full_action block
      # Set the chunk limit conservatively to avoid exceeding the recommended
      # chunk size of 5MB per write request.
      buffer_chunk_limit 512k
      # Cap the combined memory usage of this buffer and the one below to
      # 512KiB/chunk * (6 + 2) chunks = 4 MiB
      buffer_queue_limit 8
      # Never wait more than 5 seconds before flushing logs in the non-error case.
      flush_interval 5s
      # Never wait longer than 30 seconds between retries.
      max_retry_wait 30
      disable_retry_limit true
      num_threads 2
      use_grpc true
      # Skip timestamp adjustment as this is in a controlled environment with
      # known timestamp format. This helps with CPU usage.
      adjust_invalid_timestamps false
      enable_metadata_agent false
      use_metadata_service false

      vm_id ""
      vm_name ""
      zone ""
      k8s_cluster_name "minikube"
      k8s_cluster_location "minikube"
    </match>

    # Attach local_resource_id for 'k8s_node' monitored resource.
    <filter **>
      @type record_transformer
      enable_ruby true
      <record>
        "logging.googleapis.com/local_resource_id" ${"k8s_node.#{ENV['NODE_NAME']}"}
      </record>
    </filter>


metadata:
  name: fluentd-config-directory
  namespace: kube-system
